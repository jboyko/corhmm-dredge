---
title: "RegularizationBiasVariance"
author: "James Boyko"
date: "2024-04-01"
output: html_document
---

## Bias and variance in discrete character evolution

The bias-variance tradeoff is not something we usually discuss in phylogenetic comparative methods (PCMs). But, PCMs are like any statistical method in that they are trying to strike a balance between overfitting and underfitting data. Bias is typically related to underfitting in the sense that we are not able to capture important aspects of the data and generate good predictions based on our model (usually due to an overly simplistic model). Variance on the other hand is related to, and generally a consequence of, overfitting. High variance means that the model is unable to generalize to unseen data because it has confused random noise in the data for genuine signal - i.e., it is specific to the dataset at hand, but only that data. The bias-variance tradeoff is then referring to the fact that we cannot have our cake and eat it to. If we want a model that fits the data well, we are sacrificing it's generalizability. If we want something general, then we are likely to miss out on nuances in any specific dataset. We have several techniques in our repetoire that deal with this, the one I will focus on (because it's my favorite) is AIC. 