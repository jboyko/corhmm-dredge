root_index <- min(tree$edge[,1])
objective <- -log(t(liks_preorder[root_index,]) %*% liks_postorder[root_index,])
# the gradient is...
gradient <- compute_likelihood_gradient(tree = tree,
Q = Q,
liks_postorder = liks_postorder,
liks_preorder = liks_preorder)
est_gradient <- gradient[index_mat > 0]
print(objective)
print(est_gradient)
out <- list(objective = objective,
gradient = est_gradient)
return(out)
}
log_inner_product <- function(
logs) {
sum_logs <- sum(logs)
max_log <- max(sum_logs)
sum_exp <- sum(exp(sum_logs - max_log))
log_inner_prod <- max_log + log(sum_exp)
return(log_inner_prod)
}
library(corHMM)
library(expm)
# Create a random tree with 5 tips
tree <- rtree(n = 5)
# Define the transition matrix for a simple two-state model
Q <- matrix(c(-1, 1, 2, -2), ncol = 2, byrow = TRUE)
rownames(Q) <- colnames(Q) <- c("0", "1")
# Simulate character states using the transition matrix
states <- rTraitDisc(tree, model = Q, states = c("0", "1"))
# run
root.p <- c(0.5, 0.5)
n_states <- 2
n <- Ntip(tree) + Nnode(tree)
liks <- matrix(1, n, n_states)
levels <- levels(states)
for(i in seq_along(states)){
state_index <- which(levels %in% states[i])
liks[i, -state_index] <- 0
}
# test likelihoods
test <- calculateNegLogLiklihood(c(2,1), tree, liks, root.p, index_mat)
dat <- data.frame(sp=names(states), d = states)
cor_fix <- corHMM(tree, dat, 1, root.p=c(.5,.5), p=c(1,2), model = "ARD")
setNames(c(cor_fix$loglik, (-test$objective)), c("OG corHMM", "new LnLik"))
# test gradient
cor_fit <- corHMM(tree, dat, 1, root.p=c(.5,.5), model = "ARD")
opts <- list("algorithm"="NLOPT_LD_LBFGS",
"xtol_rel"=1.0e-8)
res <- nloptr( x0=c(1,1),
eval_f = calculateNegLogLiklihood,
opts = opts,
tree = tree,
liks = liks,
root.p = root.p,
index_mat = index_mat,
lb = c(1e-10,1e-10),
ub = c(100,100))
?nloptr
eval_f_list <- function(x) {
return( list( "objective" = 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2,
"gradient"  = c( -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
200 * (x[2] - x[1] * x[1])) ) )
}
# solve Rosenbrock Banana function using an objective function that
# returns a list with the objective value and its gradient
res <- nloptr( x0=x0,
eval_f=eval_f_list,
opts=opts)
eval_f_list <- function(x) {
return( list( "objective" = 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2,
"gradient"  = c( -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
200 * (x[2] - x[1] * x[1])) ) )
}
# solve Rosenbrock Banana function using an objective function that
# returns a list with the objective value and its gradient
x0 <- c( -1.2, 1 )
res <- nloptr( x0=x0,
eval_f=eval_f_list,
opts=opts)
res
eval_f_list <- function(x) {
print(x)
out <- list( "objective" = 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2,
"gradient"  = c( -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
200 * (x[2] - x[1] * x[1])) )
return( out )
}
# solve Rosenbrock Banana function using an objective function that
# returns a list with the objective value and its gradient
x0 <- c( -1.2, 1 )
res <- nloptr( x0=x0,
eval_f=eval_f_list,
opts=opts)
eval_f_list <- function(x) {
out <- list( "objective" = 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2,
"gradient"  = c( -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
200 * (x[2] - x[1] * x[1])) )
print(out$objective)
print(out$gradient)
return( out )
}
# solve Rosenbrock Banana function using an objective function that
# returns a list with the objective value and its gradient
x0 <- c( -1.2, 1 )
res <- nloptr( x0=x0,
eval_f=eval_f_list,
opts=opts)
# test likelihoods
test <- calculateNegLogLiklihood(c(2,1), tree, liks, root.p, index_mat)
# test likelihoods
test <- calculateNegLogLiklihood(c(10,1), tree, liks, root.p, index_mat)
# test likelihoods
test <- calculateNegLogLiklihood(c(100,1), tree, liks, root.p, index_mat)
# test likelihoods
test <- calculateNegLogLiklihood(c(5000,1), tree, liks, root.p, index_mat)
# test likelihoods
test <- calculateNegLogLiklihood(c(-1,1), tree, liks, root.p, index_mat)
# test likelihoods
test <- calculateNegLogLiklihood(c(0,1), tree, liks, root.p, index_mat)
# test likelihoods
test <- calculateNegLogLiklihood(c(.1,1), tree, liks, root.p, index_mat)
gradient
gradient_vector
gradient_vector
rowSums(gradient_vector)
gradient_vector
compute_likelihood_gradient <- function(
tree, Q, liks_postorder, liks_preorder){
gradient_vector <-  sapply(1:dim(tree$edge)[1], function(i)
kronecker(liks_preorder[i,], liks_postorder[i,]) *
tree$edge.length[i])
return(rowSums(gradient_vector))
}
# the gradient is...
gradient <- compute_likelihood_gradient(tree = tree,
Q = Q,
liks_postorder = liks_postorder,
liks_preorder = liks_preorder)
gradient
index_mat
est_gradient
gradient
# the gradient is...
vec_D <- compute_likelihood_gradient(tree = tree,
Q = Q,
liks_postorder = liks_postorder,
liks_preorder = liks_preorder)
M <- matrix(0, max(index_mat), dim(index_mat)^2)
M
M[1,1] <- -1
M[1,2] <- 1
M[2,3] <- 1
M[3,4] <- -1
M[2,4] <- -1
M
M %*% vec_D
gradient <- c(M %*% vec_D)
gradient
print(objective)
print(gradient)
out <- list(objective = objective,
gradient = gradient)
calculateNegLogLiklihood <- function(p, tree, liks, root.p, index_mat){
print(p)
index_mat[is.na(index_mat)] <- 0
Q <- matrix(0, nrow=nrow(index_mat), ncol=ncol(index_mat))
Q[index_mat > 0] <- p[index_mat[index_mat > 0]]
diag(Q) <- -rowSums(Q)
# pre calculate all necessary matrix expm
p_mat_by_edge <- sapply(tree$edge.length, function(x) expm(Q*x, method="Ward77"))
# get postorder partial likelihoods
liks_postorder <- compute_postorder_likelihoods(tree = tree,
liks = liks,
p_mat_by_edge = p_mat_by_edge)
# the first preorder partial likelihood is the root prior
liks[min(tree$edge[,1]), ] <- root.p
# get the preorder partial likelihoods
liks_preorder <- compute_preorder_likelihoods(tree = tree,
liks = liks,
liks_postorder = liks_postorder,
p_mat_by_edge = p_mat_by_edge)
# the liklihood is...
root_index <- min(tree$edge[,1])
objective <- -log(t(liks_preorder[root_index,]) %*% liks_postorder[root_index,])
# the gradient is...
vec_D <- compute_likelihood_gradient(tree = tree,
Q = Q,
liks_postorder = liks_postorder,
liks_preorder = liks_preorder)
M <- matrix(0, max(index_mat), dim(index_mat)^2)
M[1,1] <- -1
M[1,2] <- 1
M[2,3] <- 1
M[2,4] <- -1
gradient <- c(M %*% vec_D)
print(objective)
print(gradient)
out <- list(objective = objective,
gradient = gradient)
return(out)
}
res <- nloptr( x0=c(1,1),
eval_f = calculateNegLogLiklihood,
opts = opts,
tree = tree,
liks = liks,
root.p = root.p,
index_mat = index_mat,
lb = c(1e-10,1e-10),
ub = c(100,100))
gradient
liks_postorder
liks_preorder
# test likelihoods
test <- calculateNegLogLiklihood(c(100,1), tree, liks, root.p, index_mat)
# test likelihoods
test <- calculateNegLogLiklihood(c(1,1), tree, liks, root.p, index_mat)
cor_fix
cor_fit
index_mat
# test likelihoods
test <- calculateNegLogLiklihood(c(19,28), tree, liks, root.p, index_mat)
# test likelihoods
test <- calculateNegLogLiklihood(c(19,50), tree, liks, root.p, index_mat)
# test likelihoods
test <- calculateNegLogLiklihood(c(19,100), tree, liks, root.p, index_mat)
cor_fit
# test likelihoods
test <- calculateNegLogLiklihood(c(19.17402,28.76103), tree, liks, root.p, index_mat)
# test likelihoods
test <- calculateNegLogLiklihood(c(1,1), tree, liks, root.p, index_mat)
calculateNegLogLiklihood <- function(p, tree, liks, root.p, index_mat){
print(p)
index_mat[is.na(index_mat)] <- 0
Q <- matrix(0, nrow=nrow(index_mat), ncol=ncol(index_mat))
Q[index_mat > 0] <- p[index_mat[index_mat > 0]]
diag(Q) <- -rowSums(Q)
# pre calculate all necessary matrix expm
p_mat_by_edge <- sapply(tree$edge.length, function(x) expm(Q*x, method="Ward77"))
# get postorder partial likelihoods
liks_postorder <- compute_postorder_likelihoods(tree = tree,
liks = liks,
p_mat_by_edge = p_mat_by_edge)
# the first preorder partial likelihood is the root prior
liks[min(tree$edge[,1]), ] <- root.p
# get the preorder partial likelihoods
liks_preorder <- compute_preorder_likelihoods(tree = tree,
liks = liks,
liks_postorder = liks_postorder,
p_mat_by_edge = p_mat_by_edge)
# the liklihood is...
root_index <- min(tree$edge[,1])
objective <- -log(t(liks_preorder[root_index,]) %*% liks_postorder[root_index,])
# the gradient is...
vec_D <- compute_likelihood_gradient(tree = tree,
Q = Q,
liks_postorder = liks_postorder,
liks_preorder = liks_preorder)
M <- matrix(0, max(index_mat), dim(index_mat)^2)
M[1,1] <- -1
M[1,2] <- 1
M[2,3] <- 1
M[2,4] <- -1
gradient <- c(M %*% vec_D)
print(objective)
print(gradient)
out <- list(objective = objective,
gradient = gradient*100)
return(out)
}
res <- nloptr( x0=c(1,1),
eval_f = calculateNegLogLiklihood,
opts = opts,
tree = tree,
liks = liks,
root.p = root.p,
index_mat = index_mat,
lb = c(1e-10,1e-10),
ub = c(100,100))
# test likelihoods
test <- calculateNegLogLiklihood(c(1,1), tree, liks, root.p, index_mat)
calculateNegLogLiklihood <- function(p, tree, liks, root.p, index_mat){
print(p)
index_mat[is.na(index_mat)] <- 0
Q <- matrix(0, nrow=nrow(index_mat), ncol=ncol(index_mat))
Q[index_mat > 0] <- p[index_mat[index_mat > 0]]
diag(Q) <- -rowSums(Q)
# pre calculate all necessary matrix expm
p_mat_by_edge <- sapply(tree$edge.length, function(x) expm(Q*x, method="Ward77"))
# get postorder partial likelihoods
liks_postorder <- compute_postorder_likelihoods(tree = tree,
liks = liks,
p_mat_by_edge = p_mat_by_edge)
# the first preorder partial likelihood is the root prior
liks[min(tree$edge[,1]), ] <- root.p
# get the preorder partial likelihoods
liks_preorder <- compute_preorder_likelihoods(tree = tree,
liks = liks,
liks_postorder = liks_postorder,
p_mat_by_edge = p_mat_by_edge)
# the liklihood is...
root_index <- min(tree$edge[,1])
objective <- -log(t(liks_preorder[root_index,]) %*% liks_postorder[root_index,])
# the gradient is...
vec_D <- compute_likelihood_gradient(tree = tree,
Q = Q,
liks_postorder = liks_postorder,
liks_preorder = liks_preorder)
M <- matrix(0, max(index_mat), dim(index_mat)^2)
M[1,1] <- -1
M[1,2] <- 1
M[2,3] <- 1
M[2,4] <- -1
gradient <- c(M %*% vec_D)
print(objective)
print(gradient)
out <- list(objective = objective,
gradient = gradient*100)
return(out)
}
gradient
gradient*100
calculateNegLogLiklihood <- function(p, tree, liks, root.p, index_mat){
print(p)
index_mat[is.na(index_mat)] <- 0
Q <- matrix(0, nrow=nrow(index_mat), ncol=ncol(index_mat))
Q[index_mat > 0] <- p[index_mat[index_mat > 0]]
diag(Q) <- -rowSums(Q)
# pre calculate all necessary matrix expm
p_mat_by_edge <- sapply(tree$edge.length, function(x) expm(Q*x, method="Ward77"))
# get postorder partial likelihoods
liks_postorder <- compute_postorder_likelihoods(tree = tree,
liks = liks,
p_mat_by_edge = p_mat_by_edge)
# the first preorder partial likelihood is the root prior
liks[min(tree$edge[,1]), ] <- root.p
# get the preorder partial likelihoods
liks_preorder <- compute_preorder_likelihoods(tree = tree,
liks = liks,
liks_postorder = liks_postorder,
p_mat_by_edge = p_mat_by_edge)
# the liklihood is...
root_index <- min(tree$edge[,1])
objective <- -log(t(liks_preorder[root_index,]) %*% liks_postorder[root_index,])
# the gradient is...
vec_D <- compute_likelihood_gradient(tree = tree,
Q = Q,
liks_postorder = liks_postorder,
liks_preorder = liks_preorder)
M <- matrix(0, max(index_mat), dim(index_mat)^2)
M[1,1] <- -1
M[1,2] <- 1
M[2,3] <- 1
M[2,4] <- -1
gradient <- c(M %*% vec_D)
print(objective)
print(gradient)
out <- list(objective = objective,
gradient = gradient*1000)
return(out)
}
calculateNegLogLiklihood <- function(p, tree, liks, root.p, index_mat){
print(p)
index_mat[is.na(index_mat)] <- 0
Q <- matrix(0, nrow=nrow(index_mat), ncol=ncol(index_mat))
Q[index_mat > 0] <- p[index_mat[index_mat > 0]]
diag(Q) <- -rowSums(Q)
# pre calculate all necessary matrix expm
p_mat_by_edge <- sapply(tree$edge.length, function(x) expm(Q*x, method="Ward77"))
# get postorder partial likelihoods
liks_postorder <- compute_postorder_likelihoods(tree = tree,
liks = liks,
p_mat_by_edge = p_mat_by_edge)
# the first preorder partial likelihood is the root prior
liks[min(tree$edge[,1]), ] <- root.p
# get the preorder partial likelihoods
liks_preorder <- compute_preorder_likelihoods(tree = tree,
liks = liks,
liks_postorder = liks_postorder,
p_mat_by_edge = p_mat_by_edge)
# the liklihood is...
root_index <- min(tree$edge[,1])
objective <- -log(t(liks_preorder[root_index,]) %*% liks_postorder[root_index,])
# the gradient is...
vec_D <- compute_likelihood_gradient(tree = tree,
Q = Q,
liks_postorder = liks_postorder,
liks_preorder = liks_preorder)
M <- matrix(0, max(index_mat), dim(index_mat)^2)
M[1,1] <- -1
M[1,2] <- 1
M[2,3] <- 1
M[2,4] <- -1
gradient <- c(M %*% vec_D)
out <- list(objective = objective,
gradient = gradient*1000)
print(out)
return(out)
}
res <- nloptr( x0=c(1,1),
eval_f = calculateNegLogLiklihood,
opts = opts,
tree = tree,
liks = liks,
root.p = root.p,
index_mat = index_mat,
lb = c(1e-10,1e-10),
ub = c(100,100))
calculateNegLogLiklihood <- function(p, tree, liks, root.p, index_mat){
print(p)
index_mat[is.na(index_mat)] <- 0
Q <- matrix(0, nrow=nrow(index_mat), ncol=ncol(index_mat))
Q[index_mat > 0] <- p[index_mat[index_mat > 0]]
diag(Q) <- -rowSums(Q)
# pre calculate all necessary matrix expm
p_mat_by_edge <- sapply(tree$edge.length, function(x) expm(Q*x, method="Ward77"))
# get postorder partial likelihoods
liks_postorder <- compute_postorder_likelihoods(tree = tree,
liks = liks,
p_mat_by_edge = p_mat_by_edge)
# the first preorder partial likelihood is the root prior
liks[min(tree$edge[,1]), ] <- root.p
# get the preorder partial likelihoods
liks_preorder <- compute_preorder_likelihoods(tree = tree,
liks = liks,
liks_postorder = liks_postorder,
p_mat_by_edge = p_mat_by_edge)
# the liklihood is...
root_index <- min(tree$edge[,1])
objective <- -log(t(liks_preorder[root_index,]) %*% liks_postorder[root_index,])
# the gradient is...
vec_D <- compute_likelihood_gradient(tree = tree,
Q = Q,
liks_postorder = liks_postorder,
liks_preorder = liks_preorder)
M <- matrix(0, max(index_mat), dim(index_mat)^2)
M[1,1] <- -1
M[1,2] <- 1
M[2,3] <- 1
M[2,4] <- -1
gradient <- c(M %*% vec_D)
print(objective)
print(gradient)
out <- list(objective = objective,
gradient = gradient)
return(out)
}
liks_preorder
liks_preorder[i,]
i
i=1
kronecker(liks_preorder[i,], liks_postorder[i,])
liks_preorder[1,1] * liks_postorder[1,1]
liks_preorder[1,2] * liks_postorder[1,1]
liks_preorder[1,1] * liks_postorder[1,2]
liks_preorder[1,2] * liks_postorder[1,2]
liks_preorder
liks_postorder
rowSums(gradient_vector)
gradient_vector
library(corHMM)
data(primates)
phy <- multi2di(primates[[1]])
dat <- primates[[2]]
corhmm_fit_l1 <- corHMM:::corHMMDredge(phy = phy,
data = dat,
max.rate.cat = 1,
pen_type = "l1",
root.p="yang",
get.tip.states = TRUE,
collapse = FALSE,
lambda = 1)
data(primates)
phy <- multi2di(primates[[1]])
dat <- primates[[2]]
phy
dat
library(corHMM)
data(primates)
phy <- multi2di(primates[[1]])
dat <- primates[[2]]
# Main execution
# corhmm_fit <- corHMM(phy = phy,
#                      data = dat,
#                      rate.cat = 1,
#                      root.p="yang",
#                      get.tip.states = TRUE,
#                      collapse = FALSE)
#
#
# corhmm_fit_l1 <- corHMM:::corHMMDredge(phy = phy,
#                                        data = dat,
#                                        max.rate.cat = 1,
#                                        pen_type = "l1",
#                                        root.p="yang",
#                                        get.tip.states = TRUE,
#                                        collapse = FALSE)
#
# results_1 <- corHMM:::kFoldCrossValidation(corhmm_fit_l1, 5)
# results_1$averageScore
corhmm_fit_l1 <- corHMM:::corHMMDredge(phy = phy,
data = dat,
max.rate.cat = 1,
pen_type = "l1",
root.p="yang",
get.tip.states = TRUE,
collapse = FALSE,
lambda = 1)
phy
dat
